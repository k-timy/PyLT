{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mask_trainning_with_comments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T814Ws7pHP48",
        "colab_type": "text"
      },
      "source": [
        "# Mask Training Based on Lottery Ticket Paper\n",
        "\n",
        "Pytorch Implementation of [**Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask**](https://arxiv.org/abs/1905.01067) ( from UberAI )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcF3E31G2q3",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzG5vrX2HDJ-",
        "colab_type": "text"
      },
      "source": [
        "### Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW__V9lGHnZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec13213d-22b9-4e78-f428-9806f0655df9"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_set = datasets.MNIST('./data', train=True, download=True)\n",
        "test_set = datasets.MNIST('./data', train=False, download=True)\n",
        "\n",
        "train_set_x = train_set.data.numpy() / 255.0\n",
        "test_set_x = test_set.data.numpy() / 255.0\n",
        "\n",
        "train_set_y = train_set.targets.numpy()\n",
        "test_set_y = test_set.targets.numpy()\n",
        "\n",
        "print(train_set_y[0],train_set_y.shape)\n",
        "print(train_set_x[0].shape,train_set_x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 (60000,)\n",
            "(28, 28) (60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5jif9xmHIOO",
        "colab_type": "text"
      },
      "source": [
        "### Define and Load A Sequential Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVxhD68hG_-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Sampler,SequentialSampler,BatchSampler,Dataset,DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, x_values, y_values, out_shape):\n",
        "        self.X = x_values\n",
        "        self.y = y_values\n",
        "        self.out_shape = out_shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.X))\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return (torch.as_tensor(self.X[index].reshape(self.out_shape),dtype=torch.float32),\n",
        "                torch.as_tensor(self.y[index],dtype=torch.long))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CDoDjXXHZEo",
        "colab_type": "text"
      },
      "source": [
        "## Define Custom Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQaaNfIGHgTc",
        "colab_type": "text"
      },
      "source": [
        "### Define Bernouli function with gradient\n",
        "\n",
        "As the implementation of pytorch does not support derivation and gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyBIkLEH9wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "class Bern(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom Bernouli function that supports gradients.\n",
        "    The original Pytorch implementation of Bernouli function,\n",
        "    does not support gradients.\n",
        "\n",
        "    First-Order gradient of bernouli function with prbabilty p, is p.\n",
        "\n",
        "    Inputs: Tensor of arbitrary shapes with bounded values in [0,1] interval\n",
        "    Outputs: Randomly generated Tensor of only {0,1}, given Inputs as distributions.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return torch.bernoulli(input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):      \n",
        "        pvals = ctx.saved_tensors\n",
        "        return pvals[0] * grad_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05tpKxQ-H_Lo",
        "colab_type": "text"
      },
      "source": [
        "### Define a MaskedLinear layer\n",
        "\n",
        "Which is a custom fully connected linear layer that its weights $W_f$ remain constant once initialized randomly.\n",
        "A second weight matrix $W_m$ with the same shape as $W_f$ is used for generating a binary mask. This weight matrix can be trained through backpropagation. Each unit of $W_f$ may be passed through sigmoid function to generate the $p$ value of the $Bern(p)$ function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_SbbVNxKbyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Which is a custom fully connected linear layer that its weights $W_f$ \n",
        "    remain constant once initialized randomly.\n",
        "    A second weight matrix $W_m$ with the same shape as $W_f$ is used for\n",
        "    generating a binary mask. This weight matrix can be trained through\n",
        "    backpropagation. Each unit of $W_f$ may be passed through sigmoid\n",
        "    function to generate the $p$ value of the $Bern(p)$ function.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, device=None):\n",
        "        super(MaskedLinear, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # Fully Connected Weights\n",
        "        self.fcw = torch.randn((out_features,in_features),requires_grad=False,device=device)\n",
        "        # Weights of Mask\n",
        "        self.mask = nn.Parameter(torch.randn_like(self.fcw,requires_grad=True,device=device))        \n",
        "\n",
        "    def forward(self, x):        \n",
        "        # Generate probability of bernouli distributions\n",
        "        s_m = torch.sigmoid(self.mask)\n",
        "        # Generate a binary mask based on the distributions\n",
        "        g_m = Bern.apply(s_m)\n",
        "        # Keep weights where mask is 1 and set others to 0\n",
        "        effective_weight = self.fcw * g_m            \n",
        "        # Apply the effective weight on the input data\n",
        "        lin = F.linear(x, effective_weight)\n",
        "\n",
        "        return lin\n",
        "        \n",
        "    def __str__(self):        \n",
        "        prod = torch.prod(*self.fcw.shape).item()\n",
        "        return 'Mask Layer: \\n FC Weights: {}, {}, MASK: {}'.format(self.fcw.sum(),torch.abs(self.fcw).sum(),self.mask.sum() / prod)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DxYdRJ1KkTT",
        "colab_type": "text"
      },
      "source": [
        "### Define a Custom Masked ANN\n",
        "\n",
        "A simple fully connected masked network for our test purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuro62A-NMjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskANN(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(MaskANN, self).__init__()\n",
        "        self.ml1 = MaskedLinear(784, 1200,device)        \n",
        "        self.ml2 = MaskedLinear(1200, 1200,device)\n",
        "        self.ml3 = MaskedLinear(1200,10,device)      \n",
        "\n",
        "    def forward(self, x):        \n",
        "        x = self.ml1(x)\n",
        "        x = F.relu(x)        \n",
        "        x = self.ml2(x)        \n",
        "        x = F.relu(x)        \n",
        "        x = self.ml3(x)\n",
        "                \n",
        "        return x\n",
        "\n",
        "    def get_layers(self):\n",
        "        return [self.ml1, self.ml2, self.ml3]\n",
        "\n",
        "    def print_weights(self):\n",
        "        print('FC 1: ', self.ml1.weight.sum().item(), torch.abs(self.ml1.weight).sum().item())\n",
        "        print('FC 2: ', self.ml2.weight.sum().item(), torch.abs(self.ml2.weight).sum().item())\n",
        "        print('FC 3: ', self.ml3.weight.sum().item(), torch.abs(self.ml3.weight).sum().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHk9rwPPNOW-",
        "colab_type": "text"
      },
      "source": [
        "### Defintion of application functions: train, test, and main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp5O9YdTHPVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "1c65e332-0e71-4407-f2ac-3db166af2501"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)                \n",
        "        loss = loss_func(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()                \n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss += F.cross_entropy(output, target).item() \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "mask_net = None\n",
        "\n",
        "def main():\n",
        "\n",
        "    seed = 0\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    train_ds = SpikeDataset(train_set_x,train_set_y,(784,))\n",
        "    test_ds = SpikeDataset(test_set_x,test_set_y,(784,))\n",
        "\n",
        "    batch_size = 100\n",
        "    epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    momentum = 0.1\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=False,drop_last=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_ds, batch_size=batch_size, shuffle=False,drop_last=True)\n",
        "\n",
        "    global mask_net\n",
        "    mask_net = MaskANN(device)\n",
        "    \n",
        "    #optimizer = optim.SGD(mask_net.parameters(), lr=learning_rate, momentum=momentum)    \n",
        "    optimizer = optim.Adam(mask_net.parameters())\n",
        "    \n",
        "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(mask_net, device, train_loader, optimizer, epoch)\n",
        "        test(mask_net, device, test_loader)        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter values of MaskNet:\n",
            "wayt:  tensor(750996.1250, grad_fn=<SumBackward0>)\n",
            "Train Epoch: 1 [59900/60000 (100%)]\tLoss: 1263.382080\n",
            "\n",
            "Test set: Average loss: 10.3730, Accuracy: 3942/10000 (39%)\n",
            "\n",
            "wayt:  tensor(752205., grad_fn=<SumBackward0>)\n",
            "Train Epoch: 2 [59900/60000 (100%)]\tLoss: 547.427246\n",
            "\n",
            "Test set: Average loss: 6.6777, Accuracy: 5324/10000 (53%)\n",
            "\n",
            "wayt:  tensor(753496., grad_fn=<SumBackward0>)\n",
            "Train Epoch: 3 [59900/60000 (100%)]\tLoss: 318.329987\n",
            "\n",
            "Test set: Average loss: 4.8698, Accuracy: 6226/10000 (62%)\n",
            "\n",
            "wayt:  tensor(754822.0625, grad_fn=<SumBackward0>)\n",
            "Train Epoch: 4 [59900/60000 (100%)]\tLoss: 292.990753\n",
            "\n",
            "Test set: Average loss: 3.8178, Accuracy: 6768/10000 (68%)\n",
            "\n",
            "wayt:  tensor(756206.2500, grad_fn=<SumBackward0>)\n",
            "Train Epoch: 5 [59900/60000 (100%)]\tLoss: 301.551514\n",
            "\n",
            "Test set: Average loss: 3.0681, Accuracy: 7172/10000 (72%)\n",
            "\n",
            "wayt:  tensor(757582.5000, grad_fn=<SumBackward0>)\n",
            "Train Epoch: 6 [59900/60000 (100%)]\tLoss: 182.448242\n",
            "\n",
            "Test set: Average loss: 2.7339, Accuracy: 7462/10000 (75%)\n",
            "\n",
            "wayt:  tensor(759038.6250, grad_fn=<SumBackward0>)\n",
            "Train Epoch: 7 [59900/60000 (100%)]\tLoss: 139.039658\n",
            "\n",
            "Test set: Average loss: 2.3422, Accuracy: 7587/10000 (76%)\n",
            "\n",
            "wayt:  tensor(760453.0625, grad_fn=<SumBackward0>)\n",
            "Train Epoch: 8 [59900/60000 (100%)]\tLoss: 222.919449\n",
            "\n",
            "Test set: Average loss: 2.1755, Accuracy: 7732/10000 (77%)\n",
            "\n",
            "wayt:  tensor(761976.5000, grad_fn=<SumBackward0>)\n",
            "Train Epoch: 9 [59900/60000 (100%)]\tLoss: 160.604858\n",
            "\n",
            "Test set: Average loss: 1.7397, Accuracy: 7974/10000 (80%)\n",
            "\n",
            "wayt:  tensor(763464., grad_fn=<SumBackward0>)\n",
            "Train Epoch: 10 [59900/60000 (100%)]\tLoss: 193.334290\n",
            "\n",
            "Test set: Average loss: 1.6303, Accuracy: 7936/10000 (79%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}